{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-20T16:51:25.158060Z",
     "start_time": "2024-12-20T16:51:25.079384Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional, Iterator\n",
    "import logging\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import threading\n",
    "import queue\n",
    "from abc import ABC, abstractmethod\n",
    "import hashlib\n",
    "import psutil"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T16:51:26.908935Z",
     "start_time": "2024-12-20T16:51:25.201654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuração de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ProcessingMetrics:\n",
    "    \"\"\"Classe para rastreamento de métricas de processamento\"\"\"\n",
    "    def __init__(self, processing_time: float, memory_usage: float,\n",
    "                 throughput: float, error_rate: float, batch_size: int):\n",
    "        self.processing_time = processing_time\n",
    "        self.memory_usage = memory_usage\n",
    "        self.throughput = throughput\n",
    "        self.error_rate = error_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "class DataChunk:\n",
    "    \"\"\"Classe para gerenciamento eficiente de chunks de dados\"\"\"\n",
    "    def __init__(self, data: np.ndarray, metadata: Dict):\n",
    "        self._data = data\n",
    "        self._view = None\n",
    "        self._metadata = metadata\n",
    "        self._hash = None\n",
    "\n",
    "    @property\n",
    "    def data(self) -> np.ndarray:\n",
    "        \"\"\"Retorna view dos dados para economia de memória\"\"\"\n",
    "        if self._view is None:\n",
    "            self._view = self._data.view()\n",
    "        return self._view\n",
    "\n",
    "    @property\n",
    "    def hash(self) -> str:\n",
    "        \"\"\"Calcula hash do chunk para verificação de integridade\"\"\"\n",
    "        if self._hash is None:\n",
    "            self._hash = hashlib.md5(self._data.tobytes()).hexdigest()\n",
    "        return self._hash\n",
    "\n",
    "    def process_inplace(self, func) -> None:\n",
    "        \"\"\"Processa dados in-place para economia de memória\"\"\"\n",
    "        func(self._data)\n",
    "        self._view = None  # Invalida view atual\n",
    "        self._hash = None  # Invalida hash atual\n",
    "\n",
    "class DataProcessor(ABC):\n",
    "    \"\"\"Classe base abstrata para processadores de dados\"\"\"\n",
    "    @abstractmethod\n",
    "    def process(self, chunk: DataChunk) -> DataChunk:\n",
    "        pass\n",
    "\n",
    "class StreamProcessor:\n",
    "    \"\"\"\n",
    "    Processador de streams de dados otimizado para grandes volumes\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 chunk_size: int = 1000,\n",
    "                 max_workers: int = 4,\n",
    "                 buffer_size: int = 1000):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.max_workers = max_workers\n",
    "        self.buffer_size = buffer_size\n",
    "        self.metrics = []\n",
    "        self.processing_queue = queue.Queue(maxsize=buffer_size)\n",
    "        self.result_queue = queue.Queue(maxsize=buffer_size)\n",
    "        self._stop_event = threading.Event()\n",
    "\n",
    "    def _initialize_workers(self) -> None:\n",
    "        \"\"\"Inicializa pool de workers para processamento paralelo\"\"\"\n",
    "        self.thread_pool = ThreadPoolExecutor(max_workers=self.max_workers)\n",
    "        self.process_pool = ProcessPoolExecutor(max_workers=self.max_workers)\n",
    "\n",
    "    def _create_optimized_chunk(self,\n",
    "                              data: np.ndarray,\n",
    "                              metadata: Optional[Dict] = None) -> DataChunk:\n",
    "        \"\"\"Cria chunk otimizado com metadata\"\"\"\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "        return DataChunk(data, metadata)\n",
    "\n",
    "    def _process_chunk_with_metrics(self,\n",
    "                                  chunk: DataChunk,\n",
    "                                  processor: DataProcessor) -> Tuple[DataChunk, ProcessingMetrics]:\n",
    "        \"\"\"Processa chunk individual com métricas\"\"\"\n",
    "        start_time = time.time()\n",
    "        start_memory = self._get_memory_usage()\n",
    "\n",
    "        try:\n",
    "            result = processor.process(chunk)\n",
    "\n",
    "            end_time = time.time()\n",
    "            end_memory = self._get_memory_usage()\n",
    "            processing_time = end_time - start_time\n",
    "            memory_delta = end_memory - start_memory\n",
    "            throughput = len(chunk.data) / processing_time\n",
    "\n",
    "            metrics = ProcessingMetrics(\n",
    "                processing_time=processing_time,\n",
    "                memory_usage=memory_delta,\n",
    "                throughput=throughput,\n",
    "                error_rate=0.0,\n",
    "                batch_size=len(chunk.data)\n",
    "            )\n",
    "\n",
    "            return result, metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro no processamento: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _optimize_memory_usage(self, chunk: DataChunk) -> DataChunk:\n",
    "        \"\"\"Otimiza uso de memória do chunk\"\"\"\n",
    "        if not chunk.data.flags.owndata:\n",
    "            return chunk\n",
    "\n",
    "        optimized_data = np.ascontiguousarray(chunk.data)\n",
    "        return self._create_optimized_chunk(optimized_data, chunk._metadata)\n",
    "\n",
    "    def process_stream(self,\n",
    "                      data_generator: Iterator[np.ndarray],\n",
    "                      processor: DataProcessor) -> Iterator[DataChunk]:\n",
    "        \"\"\"Processa stream de dados de forma otimizada\"\"\"\n",
    "        self._initialize_workers()\n",
    "\n",
    "        def producer():\n",
    "            try:\n",
    "                for data_batch in data_generator:\n",
    "                    if self._stop_event.is_set():\n",
    "                        break\n",
    "                    chunk = self._create_optimized_chunk(data_batch)\n",
    "                    chunk = self._optimize_memory_usage(chunk)\n",
    "                    self.processing_queue.put(chunk)\n",
    "            finally:\n",
    "                self.processing_queue.put(None)\n",
    "\n",
    "        def consumer():\n",
    "            while True:\n",
    "                chunk = self.processing_queue.get()\n",
    "                if chunk is None:\n",
    "                    self.result_queue.put(None)\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    result, metrics = self._process_chunk_with_metrics(chunk, processor)\n",
    "                    self.metrics.append(metrics)\n",
    "                    self.result_queue.put(result)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Erro no consumidor: {str(e)}\")\n",
    "                    self._stop_event.set()\n",
    "                    raise\n",
    "                finally:\n",
    "                    self.processing_queue.task_done()\n",
    "\n",
    "        producer_thread = threading.Thread(target=producer)\n",
    "        consumer_thread = threading.Thread(target=consumer)\n",
    "\n",
    "        producer_thread.start()\n",
    "        consumer_thread.start()\n",
    "\n",
    "        while True:\n",
    "            result = self.result_queue.get()\n",
    "            if result is None:\n",
    "                break\n",
    "            yield result\n",
    "            self.result_queue.task_done()\n",
    "\n",
    "        producer_thread.join()\n",
    "        consumer_thread.join()\n",
    "        self.thread_pool.shutdown()\n",
    "        self.process_pool.shutdown()\n",
    "\n",
    "    def get_processing_metrics(self) -> List[ProcessingMetrics]:\n",
    "        \"\"\"Retorna métricas de processamento\"\"\"\n",
    "        return self.metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_memory_usage() -> float:\n",
    "        \"\"\"Obtém uso atual de memória\"\"\"\n",
    "        process = psutil.Process()\n",
    "        return process.memory_info().rss / 1024 / 1024  # MB\n",
    "\n",
    "class OptimizedDataTransformer:\n",
    "    \"\"\"Implementa transformações otimizadas em dados\"\"\"\n",
    "    def __init__(self):\n",
    "        self._cached_views = {}\n",
    "\n",
    "    def transform_batch(self,\n",
    "                       data: np.ndarray,\n",
    "                       operations: List[str]) -> np.ndarray:\n",
    "        \"\"\"Aplica sequência de transformações em batch de dados\"\"\"\n",
    "        current = data.view()\n",
    "\n",
    "        for op in operations:\n",
    "            if op == 'normalize':\n",
    "                mean = current.mean()\n",
    "                std = current.std()\n",
    "                result = current.copy()\n",
    "                for idx in np.ndindex(current.shape):\n",
    "                    result.itemset(idx, (current.item(idx) - mean) / std)\n",
    "                current = result\n",
    "\n",
    "            elif op == 'filter_outliers':\n",
    "                q1, q3 = np.percentile(current, [25, 75])\n",
    "                iqr = q3 - q1\n",
    "                mask = ((current >= (q1 - 1.5 * iqr)) &\n",
    "                       (current <= (q3 + 1.5 * iqr)))\n",
    "                current = current[mask].copy()\n",
    "\n",
    "            elif op == 'fill_missing':\n",
    "                if np.any(np.isnan(current)):\n",
    "                    temp = current.copy()\n",
    "                    temp.fill(np.nanmean(current))\n",
    "                    current = temp\n",
    "\n",
    "        return current\n",
    "\n",
    "    def apply_rolling_operation(self,\n",
    "                              data: np.ndarray,\n",
    "                              window_size: int,\n",
    "                              operation: str) -> np.ndarray:\n",
    "        \"\"\"Aplica operação em janela móvel de forma otimizada\"\"\"\n",
    "        result = np.zeros(len(data) - window_size + 1)\n",
    "\n",
    "        for i in range(len(result)):\n",
    "            window = data[i:i + window_size].view()\n",
    "\n",
    "            if operation == 'mean':\n",
    "                result[i] = window.mean()\n",
    "            elif operation == 'std':\n",
    "                result[i] = window.std()\n",
    "            elif operation == 'sum':\n",
    "                result[i] = window.sum()\n",
    "\n",
    "        return result\n",
    "\n",
    "class SampleProcessor(DataProcessor):\n",
    "    \"\"\"Processador de exemplo para demonstração\"\"\"\n",
    "    def process(self, chunk: DataChunk) -> DataChunk:\n",
    "        transformer = OptimizedDataTransformer()\n",
    "        processed_data = transformer.transform_batch(\n",
    "            chunk.data,\n",
    "            operations=['normalize', 'filter_outliers', 'fill_missing']\n",
    "        )\n",
    "        return DataChunk(processed_data, chunk._metadata)\n",
    "\n",
    "def generate_sample_data(n_samples: int = 1000000):\n",
    "    \"\"\"Gera dados de exemplo para processamento\"\"\"\n",
    "    for _ in range(0, n_samples, 1000):\n",
    "        yield np.random.randn(1000)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processor = StreamProcessor(chunk_size=1000, max_workers=4)\n",
    "    sample_processor = SampleProcessor()\n",
    "\n",
    "    logger.info(\"Iniciando processamento...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    processed_chunks = []\n",
    "    for chunk in processor.process_stream(\n",
    "        generate_sample_data(1000000),\n",
    "        sample_processor\n",
    "    ):\n",
    "        processed_chunks.append(chunk)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    metrics = processor.get_processing_metrics()\n",
    "    avg_throughput = np.mean([m.throughput for m in metrics])\n",
    "    avg_memory = np.mean([m.memory_usage for m in metrics])\n",
    "\n",
    "    logger.info(f\"Processamento concluído em {total_time:.2f} segundos\")\n",
    "    logger.info(f\"Throughput médio: {avg_throughput:.2f} registros/s\")\n",
    "    logger.info(f\"Uso médio de memória: {avg_memory:.2f} MB\")"
   ],
   "id": "5893d5907903e390",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 13:51:25,241 - __main__ - INFO - Iniciando processamento...\n",
      "2024-12-20 13:51:26,904 - __main__ - INFO - Processamento concluído em 1.66 segundos\n",
      "2024-12-20 13:51:26,906 - __main__ - INFO - Throughput médio: 1065799.10 registros/s\n",
      "2024-12-20 13:51:26,907 - __main__ - INFO - Uso médio de memória: 0.01 MB\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-24T03:02:42.310293Z",
     "start_time": "2024-12-24T03:02:42.307693Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from queue import Queue\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, gamma\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T03:02:43.144113Z",
     "start_time": "2024-12-24T03:02:42.367312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuração de logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                   format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SimulationConfig:\n",
    "    \"\"\"Configuração para simulação Monte Carlo\"\"\"\n",
    "    def __init__(self, n_simulations: int, batch_size: int, n_variables: int,\n",
    "                 confidence_level: float, optimization_target: float):\n",
    "        self.n_simulations = n_simulations\n",
    "        self.batch_size = batch_size\n",
    "        self.n_variables = n_variables\n",
    "        self.confidence_level = confidence_level\n",
    "        self.optimization_target = optimization_target\n",
    "\n",
    "class Distribution:\n",
    "    \"\"\"Classe base para distribuições probabilísticas\"\"\"\n",
    "    def sample(self, size: int) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def params(self) -> dict:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class OptimizedNormalDistribution(Distribution):\n",
    "    \"\"\"Distribuição normal otimizada\"\"\"\n",
    "    def __init__(self, mu: float, sigma: float, rng: np.random.Generator):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.rng = rng\n",
    "        self._cached_samples = None\n",
    "        self._cache_size = 10000\n",
    "\n",
    "    def sample(self, size: int) -> np.ndarray:\n",
    "        if self._cached_samples is None or len(self._cached_samples) < size:\n",
    "            self._cached_samples = self.rng.normal(\n",
    "                self.mu, self.sigma, self._cache_size\n",
    "            )\n",
    "\n",
    "        result = self._cached_samples[:size]\n",
    "        self._cached_samples = self._cached_samples[size:]\n",
    "        return result\n",
    "\n",
    "    def params(self) -> dict:\n",
    "        return {'mu': self.mu, 'sigma': self.sigma}\n",
    "\n",
    "class SimulationResult:\n",
    "    \"\"\"Resultados da simulação com métricas\"\"\"\n",
    "    def __init__(self, data: np.ndarray, metrics: Dict[str, float]):\n",
    "        self.data = data\n",
    "        self.metrics = metrics\n",
    "        self.timestamp = time.time()\n",
    "\n",
    "    def get_confidence_interval(self, confidence_level: float) -> Tuple[float, float]:\n",
    "        \"\"\"Calcula intervalo de confiança\"\"\"\n",
    "        z_score = norm.ppf((1 + confidence_level) / 2)\n",
    "        mean = np.mean(self.data)\n",
    "        std = np.std(self.data) / np.sqrt(len(self.data))\n",
    "        return mean - z_score * std, mean + z_score * std\n",
    "\n",
    "class MonteCarloEngine:\n",
    "    \"\"\"Motor de simulação Monte Carlo otimizado\"\"\"\n",
    "    def __init__(self, config: SimulationConfig):\n",
    "        self.config = config\n",
    "        self.rng = np.random.default_rng(seed=42)\n",
    "        self.results_queue = Queue()\n",
    "        self.distributions: List[Distribution] = []\n",
    "        self._initialize_distributions()\n",
    "\n",
    "    def _initialize_distributions(self):\n",
    "        \"\"\"Inicializa distribuições para simulação\"\"\"\n",
    "        for _ in range(self.config.n_variables):\n",
    "            mu = self.rng.normal(0, 1)\n",
    "            sigma = self.rng.uniform(0.5, 2)\n",
    "            self.distributions.append(\n",
    "                OptimizedNormalDistribution(mu, sigma, self.rng)\n",
    "            )\n",
    "\n",
    "    def _simulate_batch(self, batch_size: int) -> np.ndarray:\n",
    "        \"\"\"Simula um lote de dados\"\"\"\n",
    "        results = np.zeros((batch_size, len(self.distributions)))\n",
    "        for i, dist in enumerate(self.distributions):\n",
    "            results[:, i] = dist.sample(batch_size)\n",
    "        return results\n",
    "\n",
    "    def _process_batch_results(self, batch_data: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Processa resultados do lote\"\"\"\n",
    "        return {\n",
    "            'mean': float(np.mean(batch_data)),\n",
    "            'std': float(np.std(batch_data)),\n",
    "            'var': float(np.var(batch_data)),\n",
    "            'skew': float(gamma.stats(a=3, moments='s')),  # Exemplo simplificado\n",
    "            'kurtosis': float(gamma.stats(a=3, moments='k'))  # Exemplo simplificado\n",
    "        }\n",
    "\n",
    "    def _optimize_parameters(self, batch_results: List[SimulationResult]) -> Dict[str, float]:\n",
    "        \"\"\"Otimiza parâmetros baseado nos resultados\"\"\"\n",
    "        all_data = np.concatenate([r.data for r in batch_results])\n",
    "        target = self.config.optimization_target\n",
    "\n",
    "        # Otimização usando gradiente descendente simplificado\n",
    "        learning_rate = 0.01\n",
    "        n_iterations = 100\n",
    "        params = np.ones(all_data.shape[1]) / all_data.shape[1]\n",
    "\n",
    "        for _ in range(n_iterations):\n",
    "            predicted = np.sum(params * all_data, axis=1)\n",
    "            error = predicted - target\n",
    "            gradient = 2 * np.mean(error[:, np.newaxis] * all_data, axis=0)\n",
    "            params -= learning_rate * gradient\n",
    "            params = np.clip(params, 0, 1)\n",
    "            params /= np.sum(params)\n",
    "\n",
    "        return {'optimal_weights': params}\n",
    "\n",
    "    def run_simulation(self) -> List[SimulationResult]:\n",
    "        \"\"\"Executa simulação Monte Carlo completa\"\"\"\n",
    "        logger.info(\"Iniciando simulação Monte Carlo...\")\n",
    "        results = []\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for i in range(0, self.config.n_simulations, self.config.batch_size):\n",
    "                batch_size = min(\n",
    "                    self.config.batch_size,\n",
    "                    self.config.n_simulations - i\n",
    "                )\n",
    "                futures.append(\n",
    "                    executor.submit(self._simulate_batch, batch_size)\n",
    "                )\n",
    "\n",
    "            for future in futures:\n",
    "                batch_data = future.result()\n",
    "                metrics = self._process_batch_results(batch_data)\n",
    "                results.append(SimulationResult(batch_data, metrics))\n",
    "\n",
    "        logger.info(\"Simulação concluída. Otimizando parâmetros...\")\n",
    "        optimization_results = self._optimize_parameters(results)\n",
    "        logger.info(f\"Otimização concluída: {optimization_results}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "class SimulationAnalyzer:\n",
    "    \"\"\"Analisador de resultados da simulação\"\"\"\n",
    "    def __init__(self, results: List[SimulationResult]):\n",
    "        self.results = results\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def compute_aggregate_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"Computa métricas agregadas\"\"\"\n",
    "        all_data = np.concatenate([r.data for r in self.results])\n",
    "        scaled_data = self.scaler.fit_transform(all_data)\n",
    "\n",
    "        return {\n",
    "            'mean': float(np.mean(all_data)),\n",
    "            'std': float(np.std(all_data)),\n",
    "            'correlation_matrix': np.corrcoef(all_data.T),\n",
    "            'principal_components': np.linalg.svd(scaled_data)[1]\n",
    "        }\n",
    "\n",
    "    def plot_distributions(self):\n",
    "        \"\"\"Plota distribuições resultantes\"\"\"\n",
    "        all_data = np.concatenate([r.data for r in self.results])\n",
    "\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i in range(all_data.shape[1]):\n",
    "            plt.subplot(2, 2, i + 1)\n",
    "            plt.hist(all_data[:, i], bins=50, density=True, alpha=0.7)\n",
    "            plt.title(f'Distribuição Variável {i+1}')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_correlation_heatmap(self):\n",
    "        \"\"\"Plota mapa de calor de correlações\"\"\"\n",
    "        all_data = np.concatenate([r.data for r in self.results])\n",
    "        corr_matrix = np.corrcoef(all_data.T)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(corr_matrix, cmap='coolwarm', aspect='auto')\n",
    "        plt.colorbar()\n",
    "        plt.title('Matriz de Correlação')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    # Configuração da simulação\n",
    "    config = SimulationConfig(\n",
    "        n_simulations=100000,\n",
    "        batch_size=1000,\n",
    "        n_variables=4,\n",
    "        confidence_level=0.95,\n",
    "        optimization_target=1.0\n",
    "    )\n",
    "\n",
    "    # Executa simulação\n",
    "    engine = MonteCarloEngine(config)\n",
    "    results = engine.run_simulation()\n",
    "\n",
    "    # Analisa resultados\n",
    "    analyzer = SimulationAnalyzer(results)\n",
    "    metrics = analyzer.compute_aggregate_metrics()\n",
    "\n",
    "    # Visualiza resultados\n",
    "    analyzer.plot_distributions()\n",
    "    analyzer.plot_correlation_heatmap()\n",
    "\n",
    "    # Imprime métricas\n",
    "    logger.info(\"Métricas Finais:\")\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            logger.info(f\"{key}:\\n{value}\")\n",
    "        else:\n",
    "            logger.info(f\"{key}: {value}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "8c154806f99b1ac7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 00:02:42,386 - INFO - Iniciando simulação Monte Carlo...\n",
      "2024-12-24 00:02:42,429 - INFO - Simulação concluída. Otimizando parâmetros...\n",
      "2024-12-24 00:02:42,981 - INFO - Otimização concluída: {'optimal_weights': array([0.59760911, 0.35254718, 0.        , 0.04984371])}\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 74.5 GiB for an array with shape (100000, 100000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 216\u001B[0m\n\u001B[1;32m    213\u001B[0m             logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 216\u001B[0m     main()\n",
      "Cell \u001B[0;32mIn[20], line 201\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# Analisa resultados\u001B[39;00m\n\u001B[1;32m    200\u001B[0m analyzer \u001B[38;5;241m=\u001B[39m SimulationAnalyzer(results)\n\u001B[0;32m--> 201\u001B[0m metrics \u001B[38;5;241m=\u001B[39m analyzer\u001B[38;5;241m.\u001B[39mcompute_aggregate_metrics()\n\u001B[1;32m    203\u001B[0m \u001B[38;5;66;03m# Visualiza resultados\u001B[39;00m\n\u001B[1;32m    204\u001B[0m analyzer\u001B[38;5;241m.\u001B[39mplot_distributions()\n",
      "Cell \u001B[0;32mIn[20], line 157\u001B[0m, in \u001B[0;36mSimulationAnalyzer.compute_aggregate_metrics\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    150\u001B[0m all_data \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate([r\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults])\n\u001B[1;32m    151\u001B[0m scaled_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler\u001B[38;5;241m.\u001B[39mfit_transform(all_data)\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mfloat\u001B[39m(np\u001B[38;5;241m.\u001B[39mmean(all_data)),\n\u001B[1;32m    155\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstd\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mfloat\u001B[39m(np\u001B[38;5;241m.\u001B[39mstd(all_data)),\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcorrelation_matrix\u001B[39m\u001B[38;5;124m'\u001B[39m: np\u001B[38;5;241m.\u001B[39mcorrcoef(all_data\u001B[38;5;241m.\u001B[39mT),\n\u001B[0;32m--> 157\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprincipal_components\u001B[39m\u001B[38;5;124m'\u001B[39m: np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39msvd(scaled_data)[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    158\u001B[0m }\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/numpy/linalg/linalg.py:1681\u001B[0m, in \u001B[0;36msvd\u001B[0;34m(a, full_matrices, compute_uv, hermitian)\u001B[0m\n\u001B[1;32m   1678\u001B[0m         gufunc \u001B[38;5;241m=\u001B[39m _umath_linalg\u001B[38;5;241m.\u001B[39msvd_n_s\n\u001B[1;32m   1680\u001B[0m signature \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mD->DdD\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m isComplexType(t) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124md->ddd\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m-> 1681\u001B[0m u, s, vh \u001B[38;5;241m=\u001B[39m gufunc(a, signature\u001B[38;5;241m=\u001B[39msignature, extobj\u001B[38;5;241m=\u001B[39mextobj)\n\u001B[1;32m   1682\u001B[0m u \u001B[38;5;241m=\u001B[39m u\u001B[38;5;241m.\u001B[39mastype(result_t, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1683\u001B[0m s \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mastype(_realType(result_t), copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mMemoryError\u001B[0m: Unable to allocate 74.5 GiB for an array with shape (100000, 100000) and data type float64"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
